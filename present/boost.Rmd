---
title: "Boosting"
#output: revealjs::revealjs_presentation
output:
  revealjs::revealjs_presentation:
    #theme: league
    theme: sky
    highlights: pygments
    transition: none
    slide_level: 2
    reveal-plugins: notes, chalkboard
    self_contained: FALSE
    center: true

---
# Boosting

## The basic challenge

+ Take a weak learner (one slightly better than random guessing)
+ Turn it into a strong learner (one with arbitrary accuracy)

## The learning challenge

+ For a target _y_ and a predictor _p(x)_, define a loss function $L(y,p(x))$. 
+ You want to find the predictor $p(x)$ that minimizes $L(y,p(x))$
+ The optimal predictor $p(x)$ will be "close" to the observed data _y_ in the sense of minimizing this loss.

## Concept is nothing new

+ Take $L(y, p(x)) = (y - p(x))^2$
+ Constrain the predictor to the form $p(x) = \beta_0 + \beta_1 x_1 + \beta_2x_2$
+ The optimal predictor is the usual linear regression line. 
+ You're finding $\beta$'s to mininize $$\sum_{i=1}^n (y_i - \beta_0 -\beta_1x_{1i} - \beta_2 x_{2i} - \beta_3x_{3i})^2$$
+ This has a __known, closed-form solution__. 

## Boosting

+ Start with a predictor $p_0(x)$.
+ If we could, we would find a function $h(x)$ so that $$p_0(x) + h(x) = y$$
+ This would mean that $h(x) = y - p_0(x)$, i.e., the residual. 
+ We could estimate $h(x)$ by some means to get $\hat{h}(x)$
+ We could then take $p_1(x) = p_0(x) + \hat{h}(x)$ to be our next candidate predictor
+ Repeat this process
+ At each stage, you're __boosting__ your previous predictor in some way to improve it.

## Why gradient boosting?

+ Recognize that $y-p$ is just the gradient of $\frac{1}{2}(y-p)^2$, which is the squared error loss. 
+ So to generalize this, we could choose a different loss function, and find $h(x)$ to estimate the gradient at that point.




## Conceptually

+ Once we fit a learner, we keep its prediction
+ We want to improve the prediction by seeing where it went wrong
+ Predict the degree to which it went wrong
+ Correct the original predictions

## The algorithm

+ Start with a training data $(x_i, y_i)$ and a differnetiable loss function $L(y,p)$
+ Start with a simple model where the prediction $p(x) = \bar{y}$
+ For m = 1 to M
    
  + Compute the _pseudo-residuals_ which is the negative gradient of the loss function: 
  $r_i = \left.\frac{\partial L(y,p)}{\partial p}\right|_{p=p_m(x)}$
  + Fit a model $h(x)$ to $r_i$
  + Update the model as $p_{m+1}(x) = p_m(x) + \lambda h(x)$ where $\lambda$ is the _learning rate_

## Let's see how this might work

# Gradient descent

## Gradient descent

+ Gradient descent is a way to minimize a function
+ The idea is to follow a curve down its __steepest__ path

## Gradient descent

```{r, echo=F}
x = sort(runif(100, -1,1))
y = x^2
grad = function(x) 2*x
x0 = 0.7
y0 = x0^2
y1 = y0 + grad(x0)*(x-x0)
plot(x,y, type='l')
lines(x,y1,lwd=2, col='red')
points(x0,y0, pch=3)
```

## Gradient Descent

![](bowl.jpg)

## Gradient Descent {data-background=#fffff0}

![](gradient_descent.png)

## Gradient descent

+ Sometimes the "learning rate" $\lambda$ is added, so you don't move the whole way
+ $F_k(x) = F_{k-1}(x) + \lambda h(x)$
+ This is to slow things down

+ If $\lambda$ is small and $M$ is large, then you will get to minimum.

# Loss functions

## Gradient descent

+ Conceptually think of residuals
+ However, in reality, think in direction of gradient

## Loss function minimization

+ We want to minimize a chosen loss function $L(y, X)$
+ The gradient of this loss function is $\frac{d L(y,X)}{d X}$
+ This is a vector with length = number of columns of X
+ Compute this at each point
+ Move in that direction

## Squared error loss

+ $L(y, p) = \sum(y_i - p_i)^2/2$
+ Gradient $G(y, p) = - \sum(y_i - p_i)$
+ Note that this is essentially the residual

## Logistic loss

+ $L(y, p) = p \log(1+e^{-y}) + (1-p)\log(1+e^y)$
+ Gradient $G(y,p) = p - \frac{1}{1+e^{-y}}$

+ This is often used for binary targets, where $y\in {0,1}$, and $p$ is the predicted probabilities

# Stochastic Gradient Descent

## Stochastic Gradient Descent

+ We can overfit our data using just gradient descent
+ Instead, fit learner to random subset of data, predict on remainder

## Stochastic gradient descent

+ Also, at each stage, take a random sample of predictors
+ Fit the model using this random sample of predictors

# Gradient boosted trees

## Gradient boosted trees

+ Weak learners are decision trees (in particular regression trees)
+ Fit a regression tree to your data
+ Compute residuals or gradients at each point
+ Fit a regression tree to the residuals/gradients and predict
+ Add the predictions to the previous predictions
+ Repeat

